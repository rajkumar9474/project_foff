{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajkumar9474/project_foff/blob/main/multimodal_deepfake_detection(updated).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN_s7hvdcnKJ",
        "outputId": "daef857e-f9f1-4a60-96ed-1314d0aa8a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q '/content/drive/MyDrive/archive.zip' -d '/content/archive'"
      ],
      "metadata": {
        "id": "9hbH0xYkcrTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1b07e9-8620-4c50-d9c9-fb3245ce7bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/archive.zip, /content/drive/MyDrive/archive.zip.zip or /content/drive/MyDrive/archive.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy torch facenet-pytorch moviepy librosa opencv-python diffusers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "beXdf47ZNsl9",
        "outputId": "d62ba68b-6ef2-430b-b5a1-04b59e7d0d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow<10.3.0,>=10.2.0 (from facenet-pytorch)\n",
            "  Downloading pillow-10.2.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (2.32.3)\n",
            "Collecting torch\n",
            "  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchvision<0.18.0,>=0.17.0 (from facenet-pytorch)\n",
            "  Downloading torchvision-0.17.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (4.67.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch)\n",
            "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.11)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.30.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading facenet_pytorch-2.6.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.2.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.17.2-cp311-cp311-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, facenet-pytorch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.2.0 facenet-pytorch-2.6.0 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 torchvision-0.17.2 triton-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "4b6c2eda2fe44462bb82c51d08ac99ab"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CONFIGURATION**"
      ],
      "metadata": {
        "id": "yBTtZ9-c1MGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries that will be used later.\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# from diffusers import DDPMPipeline  # Diffusion preprocessor\n",
        "from facenet_pytorch import MTCNN\n",
        "import cv2\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
      ],
      "metadata": {
        "id": "ko_LjtuG1KfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoFileClip\n"
      ],
      "metadata": {
        "id": "kNKMwLjsun8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global configuration variables\n",
        "FRAME_SKIP = 2\n",
        "VIDEO_FRAMES = 16\n",
        "VIDEO_SIZE = 224\n",
        "AUDIO_SAMPLE_RATE = 16000\n",
        "BATCH_SIZE = 8\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# For quick testing, use a flag to disable heavy diffusion processing.\n",
        "USE_DIFFUSION = False  # Set to False for faster testing in Colab."
      ],
      "metadata": {
        "id": "0ERbPn_Dc-W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running on:\", DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyCxZLCKom2j",
        "outputId": "b288eee8-6184-4f9a-ba68-41ffebbb23bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**DATASET** **CLASS**"
      ],
      "metadata": {
        "id": "WAagqL-q1dC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FakeAVCelebDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset loader for FakeAVCeleb.\n",
        "\n",
        "    Expected structure:\n",
        "      DATA_ROOT/\n",
        "        real/     -> contains one real video (filename without \"fake\")\n",
        "        fake/    -> contains multiple fake videos\n",
        "    For testing, you can limit the number of samples.\n",
        "    \"\"\"\n",
        "    # def __init__(self, data_root, transform=None, audio_transform=None, max_samples=None, metadata_list=None):\n",
        "    #     self.data_root = data_root\n",
        "    #     self.transform = transform\n",
        "    #     self.audio_transform = audio_transform\n",
        "    #     if metadata_list is not None:\n",
        "    #         self.samples = metadata_list  # Use provided list (for splits)\n",
        "    #     else:\n",
        "    #         self.samples = []\n",
        "    #         # Process the \"real\" folder:\n",
        "    #         real_folder = os.path.join(data_root, \"real\")\n",
        "    #         if os.path.isdir(real_folder):\n",
        "    #             for subdir in os.listdir(real_folder):\n",
        "    #                 subdir_path = os.path.join(real_folder, subdir)\n",
        "    #                 if os.path.isdir(subdir_path):\n",
        "    #                     for file in os.listdir(subdir_path):\n",
        "    #                         if file.lower().endswith(\".mp4\") and \"fake\" not in file.lower():\n",
        "    #                             video_path = os.path.join(subdir_path, file)\n",
        "    #                             self.samples.append({\"video_path\": video_path, \"label\": 0})\n",
        "    #                             break\n",
        "    #         # Process the \"fake\" folder:\n",
        "    #         fake_folder = os.path.join(data_root, \"fake\")\n",
        "    #         if os.path.isdir(fake_folder):\n",
        "    #             for subdir in os.listdir(fake_folder):\n",
        "    #                 subdir_path = os.path.join(fake_folder, subdir)\n",
        "    #                 if os.path.isdir(subdir_path):\n",
        "    #                     for file in os.listdir(subdir_path):\n",
        "    #                         if file.lower().endswith(\".mp4\"):\n",
        "    #                             video_path = os.path.join(subdir_path, file)\n",
        "    #                             self.samples.append({\"video_path\": video_path, \"label\": 1})\n",
        "    #     if max_samples is not None and len(self.samples) > max_samples:\n",
        "    #         self.samples = self.samples[:max_samples]\n",
        "    #     print(f\"Total samples loaded: {len(self.samples)}\")\n",
        "\n",
        "    def __init__(self, data_root, transform=None, audio_transform=None, max_samples=None, metadata_list=None):\n",
        "        self.data_root = data_root\n",
        "        self.transform = transform\n",
        "        self.audio_transform = audio_transform\n",
        "\n",
        "        if metadata_list is not None:\n",
        "            self.samples = metadata_list  # Use provided list (for splits)\n",
        "        else:\n",
        "            self.samples = []\n",
        "\n",
        "            # Process the \"real\" folder:\n",
        "            real_folder = os.path.join(data_root, \"real\")\n",
        "            if os.path.isdir(real_folder):\n",
        "                for file in os.listdir(real_folder):\n",
        "                    if file.lower().endswith(\".mp4\") and \"fake\" not in file.lower():\n",
        "                        video_path = os.path.join(real_folder, file)\n",
        "                        self.samples.append({\"video_path\": video_path, \"label\": 0})\n",
        "\n",
        "            # Process the \"fake\" folder:\n",
        "            fake_folder = os.path.join(data_root, \"fake\")\n",
        "            if os.path.isdir(fake_folder):\n",
        "                for file in os.listdir(fake_folder):\n",
        "                    if file.lower().endswith(\".mp4\"):\n",
        "                        video_path = os.path.join(fake_folder, file)\n",
        "                        self.samples.append({\"video_path\": video_path, \"label\": 1})\n",
        "\n",
        "        if max_samples is not None and len(self.samples) > max_samples:\n",
        "            self.samples = self.samples[:max_samples]\n",
        "\n",
        "        print(f\"Total samples loaded: {len(self.samples)}\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    # def __getitem__(self, idx):\n",
        "    #     sample = self.samples[idx]\n",
        "    #     video_path = sample[\"video_path\"]\n",
        "    #     label = sample[\"label\"]\n",
        "    #     try:\n",
        "    #         video = self.extract_frames(video_path)\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"Error extracting frames from {video_path}: {e}\")\n",
        "    #         video = torch.zeros(VIDEO_FRAMES, 3, VIDEO_SIZE, VIDEO_SIZE)\n",
        "    #     try:\n",
        "    #         audio = self.extract_audio(video_path)\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"Error extracting audio from {video_path}: {e}\")\n",
        "    #         audio = torch.zeros(1, AUDIO_SAMPLE_RATE)\n",
        "    #     if self.transform and not isinstance(video, torch.Tensor):\n",
        "    #         video = self.transform(video) # Assuming your transform can handle video directly\n",
        "    #     elif self.transform: # If video is already a Tensor, loop to apply per frame\n",
        "    #         video = torch.stack([self.transform(frame) for frame in video]) # If it was a tensor then loop through frames\n",
        "    #     return video, audio, torch.tensor(label, dtype=torch.float32)\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        video_path = sample[\"video_path\"]\n",
        "        label = sample[\"label\"]\n",
        "        try:\n",
        "            video = self.extract_frames(video_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting frames from {video_path}: {e}\")\n",
        "            video = torch.zeros(VIDEO_FRAMES, 3, VIDEO_SIZE, VIDEO_SIZE)\n",
        "        try:\n",
        "            audio = self.extract_audio(video_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting audio from {video_path}: {e}\")\n",
        "            audio = torch.zeros(1, AUDIO_SAMPLE_RATE)\n",
        "        # Check if video is already a tensor and the transform is set\n",
        "        if self.transform and not isinstance(video, torch.Tensor):\n",
        "            video = self.transform(video) # Assuming your transform can handle video directly\n",
        "        # If video is already a Tensor and the transform is set, apply it frame-by-frame.\n",
        "        elif self.transform and isinstance(video, torch.Tensor):\n",
        "            video = torch.stack([self.transform(transforms.ToPILImage()(frame)) for frame in video])  # Convert to PIL Image before applying transform\n",
        "        return video, audio, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    def extract_frames(self,video_path):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame_tensor = torch.from_numpy(frame).permute(2, 0, 1).float()  # Convert to tensor\n",
        "            frame_tensor = F.interpolate(frame_tensor.unsqueeze(0), size=(VIDEO_SIZE, VIDEO_SIZE), mode='bilinear').squeeze(0)\n",
        "            frames.append(frame_tensor / 255.0)\n",
        "            if len(frames) >= VIDEO_FRAMES:\n",
        "                break\n",
        "        cap.release()\n",
        "        frames = torch.stack(frames)\n",
        "        if frames.shape[0] < VIDEO_FRAMES:\n",
        "            pad = VIDEO_FRAMES - frames.shape[0]\n",
        "            frames = torch.cat([frames, frames[-1].unsqueeze(0).repeat(pad, 1, 1, 1)], dim=0)\n",
        "        return frames\n",
        "\n",
        "    def extract_audio(self, video_path):\n",
        "        \"\"\"\n",
        "        Loads the audio from the video file using MoviePy (no torchaudio).\n",
        "        Returns a mono numpy array of shape [samples].\n",
        "        \"\"\"\n",
        "        clip = VideoFileClip(video_path)\n",
        "        if clip.audio is None:\n",
        "            # No audio track → return one second of silence\n",
        "            return np.zeros(AUDIO_SAMPLE_RATE, dtype=np.float32)\n",
        "\n",
        "        # Extract at the desired sample rate\n",
        "        audio_np = clip.audio.to_soundarray(fps=AUDIO_SAMPLE_RATE)  # [n_samples, channels]\n",
        "        clip.audio.reader.close_proc()\n",
        "        # Convert to mono by averaging channels\n",
        "        if audio_np.ndim == 2:\n",
        "            audio_mono = audio_np.mean(axis=1)\n",
        "        else:\n",
        "            audio_mono = audio_np\n",
        "        return audio_mono.astype(np.float32)\n"
      ],
      "metadata": {
        "id": "n7PBmtBe1kni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**DIFFUSION** **PREPROCESSOR**"
      ],
      "metadata": {
        "id": "Xx2vAIux1mMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionPreprocessor(nn.Module):\n",
        "    \"\"\"\n",
        "    Uses a pretrained DDPM to refine video frames.\n",
        "    Audio is left as identity (replace with DiffWave if desired).\n",
        "    \"\"\"\n",
        "    def __init__(self, for_audio=True, for_video=True, device=DEVICE):\n",
        "        super().__init__()\n",
        "        self.for_audio = for_audio\n",
        "        self.for_video = for_video\n",
        "        self.device = device\n",
        "\n",
        "        # Load pretrained DDPM (e.g., google/ddpm-celebahq-256)\n",
        "        self.ddpm = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\")\n",
        "        self.ddpm.to(device)\n",
        "        self.ddpm.enable_model_cpu_offload()\n",
        "\n",
        "    def preprocess_image_frames(self, frames):\n",
        "        \"\"\"\n",
        "        frames: [B, T, C, H, W] tensor in [0,1]\n",
        "        Returns: denoised frames (resized to 256x256 as required)\n",
        "        \"\"\"\n",
        "        B, T, C, H, W = frames.shape\n",
        "        frames = frames.to(self.device)\n",
        "        resize = transforms.Resize((256, 256))\n",
        "        denoised_frames = []\n",
        "        for t in range(T):\n",
        "            frame_t = frames[:, t]  # shape: [B, C, H, W]\n",
        "            frame_t = torch.stack([resize(img) for img in frame_t])\n",
        "            with torch.no_grad():\n",
        "                out = self.ddpm(frame_t, num_inference_steps=50)\n",
        "            denoised = out[\"sample\"]  # [B, C, 256, 256]\n",
        "            denoised_frames.append(denoised)\n",
        "        denoised_frames = torch.stack(denoised_frames, dim=1)  # [B, T, C, 256, 256]\n",
        "        return denoised_frames\n",
        "\n",
        "    def preprocess_audio_waveform(self, waveforms):\n",
        "        \"\"\"\n",
        "        waveforms: [B, samples]\n",
        "        Identity for now (can plug in DiffWave).\n",
        "        \"\"\"\n",
        "        return waveforms\n",
        "\n",
        "    def forward(self, video_frames=None, audio_waveform=None):\n",
        "        out_video = video_frames\n",
        "        out_audio = audio_waveform\n",
        "        if video_frames is not None and self.for_video:\n",
        "            out_video = self.preprocess_image_frames(video_frames)\n",
        "        if audio_waveform is not None and self.for_audio:\n",
        "            out_audio = self.preprocess_audio_waveform(audio_waveform)\n",
        "        return out_video, out_audio"
      ],
      "metadata": {
        "id": "ra0JXMW11sM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**FACE** **REGION** **EXTRACTION**"
      ],
      "metadata": {
        "id": "0RhRyJQ21zG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "class FaceRegionExtractor(nn.Module):\n",
        "    def __init__(self, device=DEVICE):\n",
        "        super().__init__()\n",
        "        self.mtcnn = MTCNN(keep_all=True, device=device, thresholds=[0.9, 0.95, 0.99])\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, frame):\n",
        "        \"\"\"\n",
        "        frame: [B, C, H, W] tensor (values in [0,1])\n",
        "        Returns two crops: one for lips and one for eyes.\n",
        "        \"\"\"\n",
        "        from PIL import Image\n",
        "        transform_to_pil = transforms.ToPILImage()\n",
        "        B = frame.shape[0]\n",
        "        lips_crops = []\n",
        "        eyes_crops = []\n",
        "        resize = T.Resize((224, 224))\n",
        "        for i in range(B):\n",
        "            img = transform_to_pil(frame[i].cpu())\n",
        "            boxes, probs, landmarks = self.mtcnn.detect(img, landmarks=True)\n",
        "            if boxes is None or landmarks is None:\n",
        "                lips_crops.append(frame[i])\n",
        "                eyes_crops.append(frame[i])\n",
        "            else:\n",
        "                lm = landmarks[0]\n",
        "                left_mouth, right_mouth = lm[3], lm[4]\n",
        "                x1 = int(left_mouth[0] - 0.2 * abs(right_mouth[0] - left_mouth[0]))\n",
        "                y1 = int(left_mouth[1] - 0.3 * abs(right_mouth[0] - left_mouth[0]))\n",
        "                x2 = int(right_mouth[0] + 0.2 * abs(right_mouth[0] - left_mouth[0]))\n",
        "                y2 = int(right_mouth[1] + 0.3 * abs(right_mouth[0] - left_mouth[0]))\n",
        "                left_eye, right_eye = lm[0], lm[1]\n",
        "                x1_e = int(min(left_eye[0], right_eye[0]) - 0.2 * abs(right_eye[0]-left_eye[0]))\n",
        "                y1_e = int(min(left_eye[1], right_eye[1]) - 0.2 * abs(right_eye[0]-left_eye[0]))\n",
        "                x2_e = int(max(left_eye[0], right_eye[0]) + 0.2 * abs(right_eye[0]-left_eye[0]))\n",
        "                y2_e = int(max(left_eye[1], right_eye[1]) + 0.2 * abs(right_eye[0]-left_eye[0]))\n",
        "                lips_crop = img.crop((x1, y1, x2, y2))\n",
        "                lips_crop = resize(lips_crop)\n",
        "                eyes_crop = img.crop((x1_e, y1_e, x2_e, y2_e))\n",
        "                eyes_crop = resize(eyes_crop)\n",
        "                to_tensor = transforms.ToTensor()\n",
        "                lips_crops.append(to_tensor(lips_crop).to(self.device))\n",
        "                eyes_crops.append(to_tensor(eyes_crop).to(self.device))\n",
        "        lips_batch = torch.stack(lips_crops, dim=0)\n",
        "        eyes_batch = torch.stack(eyes_crops, dim=0)\n",
        "        return lips_batch, eyes_batch"
      ],
      "metadata": {
        "id": "EIcaitKl15sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VIDEO** **FEATURE** **EXTRACTION**"
      ],
      "metadata": {
        "id": "Izo2Ioqp17N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Uses a pretrained ResNet-18 to extract per-frame features.\n",
        "    These features are then passed through an LSTM to obtain a video-level representation.\n",
        "    \"\"\"\n",
        "    def __init__(self, device=DEVICE):\n",
        "        super().__init__()\n",
        "        self.cnn = resnet18(pretrained=True)\n",
        "        self.cnn.fc = nn.Identity()  # remove final classification layer\n",
        "        self.device = device\n",
        "        self.to(device)\n",
        "        # LSTM to model temporal dynamics: input_dim=512 (ResNet18 output), hidden=256\n",
        "        self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=1, batch_first=True)\n",
        "\n",
        "    def forward(self, video_frames):\n",
        "        \"\"\"\n",
        "        video_frames: [B, T, C, H, W]\n",
        "        Returns: video representation of shape [B, 256]\n",
        "        \"\"\"\n",
        "        B, T, C, H, W = video_frames.shape\n",
        "        # Process each frame through CNN:\n",
        "        frames = video_frames.view(B * T, C, H, W)\n",
        "        features = self.cnn(frames)  # [B*T, 512]\n",
        "        features = features.view(B, T, 512)\n",
        "        # Pass through LSTM:\n",
        "        lstm_out, (hn, cn) = self.lstm(features)\n",
        "        # Use final hidden state as video representation:\n",
        "        video_repr = hn[-1]  # shape: [B, 256]\n",
        "        return video_repr"
      ],
      "metadata": {
        "id": "cmtWhquv2UPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**AUDIO** **FEATURE** **EXTRACTION**"
      ],
      "metadata": {
        "id": "OawDu1NE2WFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class AudioFeatureExtractorLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracts MFCCs via librosa and passes them through a Bi-LSTM.\n",
        "    \"\"\"\n",
        "    def __init__(self, device=DEVICE):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        # Bidirectional LSTM: input_dim=13 MFCC, hidden_dim=128\n",
        "        self.lstm = nn.LSTM(input_size=13, hidden_size=128,\n",
        "                            num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, audio_waveform):\n",
        "        \"\"\"\n",
        "        audio_waveform: numpy array [samples] or torch tensor [B, samples]\n",
        "        Returns: torch tensor [B, 256] (concatenated hidden states)\n",
        "        \"\"\"\n",
        "        # Ensure we have a batch dimension\n",
        "        if isinstance(audio_waveform, np.ndarray):\n",
        "            wave_np = audio_waveform[None, :]\n",
        "        else:\n",
        "            # torch tensor\n",
        "            wave_np = audio_waveform.detach().cpu().numpy()\n",
        "\n",
        "        # If multi-channel, average to mono\n",
        "        if wave_np.ndim == 2 and wave_np.shape[1] > AUDIO_SAMPLE_RATE:\n",
        "            # assume shape [B, samples]\n",
        "            pass\n",
        "        elif wave_np.ndim == 2:\n",
        "            pass\n",
        "        elif wave_np.ndim == 1:\n",
        "            wave_np = wave_np[None, :]\n",
        "\n",
        "        # Compute MFCC for each sample in batch\n",
        "        mfcc_list = []\n",
        "        for b in range(wave_np.shape[0]):\n",
        "            y = wave_np[b]\n",
        "            m = librosa.feature.mfcc(\n",
        "                y=y,\n",
        "                sr=AUDIO_SAMPLE_RATE,\n",
        "                n_mfcc=13,\n",
        "                n_fft=400,\n",
        "                hop_length=160,\n",
        "                n_mels=26\n",
        "            )  # shape: [13, time_frames]\n",
        "            # transpose to [time_frames, 13]\n",
        "            m = m.T\n",
        "            # subsample in time\n",
        "            m = m[::FRAME_SKIP, :]\n",
        "            mfcc_list.append(m)\n",
        "\n",
        "        # Stack into tensor [B, T_audio, 13]\n",
        "        mfcc_tensor = torch.tensor(np.stack(mfcc_list, axis=0), dtype=torch.float32).to(self.device)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        out, (hn, cn) = self.lstm(mfcc_tensor)\n",
        "        # hn.shape = [2 (directions), B, 128]\n",
        "        forward_h, backward_h = hn[0], hn[1]  # each [B,128]\n",
        "        audio_repr = torch.cat((forward_h, backward_h), dim=1)  # [B,256]\n",
        "        return audio_repr\n"
      ],
      "metadata": {
        "id": "RwxJbvji2ckq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINAL** **DEEPFAKE** **DETECTOR** **MODEL**"
      ],
      "metadata": {
        "id": "JzRa_wjK2d58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepfakeDetector(nn.Module):\n",
        "    def __init__(self, device=DEVICE):\n",
        "        super(DeepfakeDetector, self).__init__()\n",
        "        self.device = device\n",
        "        # Video branch: Face extraction followed by video feature extraction.\n",
        "        self.face_extractor = FaceRegionExtractor(device=device)\n",
        "        self.video_extractor = VideoFeatureExtractor(device=device)\n",
        "        # Audio branch:\n",
        "        self.audio_extractor = AudioFeatureExtractorLSTM(device=device)\n",
        "        # Fusion and classification: Fuse video and audio representations.\n",
        "        # Video repr is 256, audio repr is 256 -> fused=512.\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, video_frames, audio_waveform):\n",
        "        \"\"\"\n",
        "        video_frames: [B, T, C, H, W] with values in [0,1]\n",
        "        audio_waveform: [B, channels, samples]\n",
        "        \"\"\"\n",
        "        # Assume video_frames have already been sub-sampled externally if needed.\n",
        "        # First, process video frames with diffusion preprocessor is assumed done externally.\n",
        "\n",
        "        # Video branch:\n",
        "        # Extract face regions from each frame.\n",
        "        B, T, C, H, W = video_frames.shape\n",
        "        # Process each frame (if needed, one can loop – here, for simplicity, we assume using the whole frame):\n",
        "        # Alternatively, if you want to focus on facial regions, use the face_extractor.\n",
        "        # Here we demonstrate using it per frame:\n",
        "        processed_frames = []\n",
        "        for t in range(T):\n",
        "            frame = video_frames[:, t]  # [B, C, H, W]\n",
        "            lips, eyes = self.face_extractor(frame)  # both: [B, C, h, w]\n",
        "            # For simplicity, we concatenate the two crops along the channel dimension:\n",
        "            face_concat = torch.cat([lips, eyes], dim=1)  # shape: [B, 2*C, h, w]\n",
        "            # You could also process lips and eyes separately.\n",
        "            # For our purpose, we simply average the two crops:\n",
        "            face_avg = (lips + eyes) / 2.0\n",
        "            processed_frames.append(face_avg)\n",
        "        processed_frames = torch.stack(processed_frames, dim=1)  # [B, T, C, h, w]\n",
        "        # Now extract video temporal features via the CNN+LSTM pipeline:\n",
        "        video_repr = self.video_extractor(processed_frames)  # [B, 256]\n",
        "\n",
        "        # Audio branch:\n",
        "        audio_repr = self.audio_extractor(audio_waveform)  # [B, 256]\n",
        "\n",
        "        # Fusion:\n",
        "        fused = torch.cat([video_repr, audio_repr], dim=1)  # [B, 512]\n",
        "        out = self.classifier(fused)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1hXuQgrL2u_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAINING** **PIPELINE**"
      ],
      "metadata": {
        "id": "vzO6n_aU2xLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for training\n",
        "EPOCHS = 20  # Or any suitable number of epochs\n",
        "LEARNING_RATE = 1e-4  # Or another appropriate learning rate\n",
        "NUM_CLASSES = 2       # For binary classification"
      ],
      "metadata": {
        "id": "RODnfDBehaLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_metrics(y_true, y_pred, threshold=0.5):\n",
        "    y_pred_bin = (y_pred >= threshold).astype(int)\n",
        "    accuracy = accuracy_score(y_true, y_pred_bin)\n",
        "    precision = precision_score(y_true, y_pred_bin, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred_bin, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred_bin, zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=EPOCHS, lr=LEARNING_RATE, device=DEVICE):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for video, audio, labels in train_loader:\n",
        "            video = video.to(device)  # [B, T, C, H, W]\n",
        "            audio = audio.to(device)  # [B, channels, samples]\n",
        "            labels = labels.to(device).unsqueeze(1)  # [B, 1]\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(video, audio)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            all_preds.extend(outputs.cpu().detach().numpy().flatten().tolist())\n",
        "            all_labels.extend(labels.cpu().detach().numpy().flatten().tolist())\n",
        "        train_acc, train_prec, train_rec, train_f1 = evaluate_metrics(np.array(all_labels), np.array(all_preds))\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} Train Loss: {np.mean(train_losses):.4f} \"\n",
        "              f\"Acc: {train_acc:.4f} Prec: {train_prec:.4f} Recall: {train_rec:.4f} F1: {train_f1:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for video, audio, labels in val_loader:\n",
        "                video = video.to(device)\n",
        "                audio = audio.to(device)\n",
        "                labels = labels.to(device).unsqueeze(1)\n",
        "                outputs = model(video, audio)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_losses.append(loss.item())\n",
        "                all_preds.extend(outputs.cpu().detach().numpy().flatten().tolist())\n",
        "                all_labels.extend(labels.cpu().detach().numpy().flatten().tolist())\n",
        "        val_acc, val_prec, val_rec, val_f1 = evaluate_metrics(np.array(all_labels), np.array(all_preds))\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} Val Loss: {np.mean(val_losses):.4f} \"\n",
        "              f\"Acc: {val_acc:.4f} Prec: {val_prec:.4f} Recall: {val_rec:.4f} F1: {val_f1:.4f}\")"
      ],
      "metadata": {
        "id": "OjXj9uUu22TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TESTING**"
      ],
      "metadata": {
        "id": "134uy5Iw6G0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader, device=DEVICE):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for video, audio, labels in test_loader:\n",
        "            video = video.to(device)\n",
        "            audio = audio.to(device)\n",
        "            outputs = model(video, audio)\n",
        "            preds = (outputs > 0.5).float().cpu().numpy()\n",
        "            all_preds.extend(preds.flatten().tolist())\n",
        "            all_labels.extend(labels.cpu().numpy().flatten().tolist())\n",
        "    acc = accuracy_score(np.array(all_labels), np.array(all_preds))\n",
        "    prec = precision_score(np.array(all_labels), np.array(all_preds), zero_division=0)\n",
        "    rec = recall_score(np.array(all_labels), np.array(all_preds), zero_division=0)\n",
        "    f1 = f1_score(np.array(all_labels), np.array(all_preds), zero_division=0)\n",
        "    print(f\"Test Metrics - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "zyc1JfWc6Je9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MAIN**"
      ],
      "metadata": {
        "id": "aQqSYHpg2845"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load full metadata from a JSON file if available,\n",
        "# Or, if your dataset is folder-based, you can get the list from the dataset object.\n",
        "# For demonstration, we use the dataset's own sample list.\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/deepfake_dataset/small_dataset\"  # Update with your FakeAVCeleb path\n",
        "MAX_SAMPLES = 50  # For quick testing\n",
        "\n",
        "# Create a temporary dataset to fetch all samples.\n",
        "temp_dataset = FakeAVCelebDataset(data_root=DATA_ROOT, transform=None, audio_transform=None, max_samples=MAX_SAMPLES)\n",
        "all_samples = temp_dataset.samples\n",
        "\n",
        "# Split 70/30 train/test, and then 80/20 train/val from train.\n",
        "train_samples, test_samples = train_test_split(all_samples, test_size=0.3, random_state=42)\n",
        "train_samples, val_samples = train_test_split(train_samples, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Train samples: {len(train_samples)}\")\n",
        "print(f\"Validation samples: {len(val_samples)}\")\n",
        "print(f\"Test samples: {len(test_samples)}\")\n"
      ],
      "metadata": {
        "id": "6QQbSwEmtf2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf988e55-560c-4d32-8a68-7ecc837147b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples loaded: 40\n",
            "Train samples: 22\n",
            "Validation samples: 6\n",
            "Test samples: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define video and audio transforms (if needed)\n",
        "video_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomHorizontalFlip(p=0.5)\n",
        "])\n",
        "audio_transform = None\n",
        "\n",
        "# Create dataset objects using the splits:\n",
        "train_dataset = FakeAVCelebDataset(data_root=DATA_ROOT, metadata_list=train_samples,\n",
        "                                    transform=video_transform, audio_transform=audio_transform)\n",
        "val_dataset = FakeAVCelebDataset(data_root=DATA_ROOT, metadata_list=val_samples,\n",
        "                                  transform=video_transform, audio_transform=audio_transform)\n",
        "test_dataset = FakeAVCelebDataset(data_root=DATA_ROOT, metadata_list=test_samples,\n",
        "                                   transform=video_transform, audio_transform=audio_transform)\n",
        "\n",
        "# For Colab, set num_workers=0 to avoid multiprocessing issues.\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n"
      ],
      "metadata": {
        "id": "2puCWdgo1auj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eadbd20-d85a-431e-dce1-4af7aabf5b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples loaded: 22\n",
            "Total samples loaded: 6\n",
            "Total samples loaded: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DeepfakeDetector(device=DEVICE)\n",
        "model = model.to(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHQFhRNLtChi",
        "outputId": "8f2dd68f-5739-4e7b-c6fc-5ecea7ca0063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyFqkQ04sA10",
        "outputId": "06d14f07-1f2d-4346-9ebc-e254e8c985fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 Train Loss: 0.7013 Acc: 0.4091 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 1/20 Val Loss: 0.7008 Acc: 0.6667 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 2/20 Train Loss: 0.6608 Acc: 0.4091 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 2/20 Val Loss: 0.7292 Acc: 0.6667 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 3/20 Train Loss: 0.6248 Acc: 0.4091 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 3/20 Val Loss: 0.7659 Acc: 0.6667 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 4/20 Train Loss: 0.6098 Acc: 0.4545 Prec: 1.0000 Recall: 0.0769 F1: 0.1429\n",
            "Epoch 4/20 Val Loss: 0.7975 Acc: 0.6667 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 5/20 Train Loss: 0.5695 Acc: 0.5909 Prec: 1.0000 Recall: 0.3077 F1: 0.4706\n",
            "Epoch 5/20 Val Loss: 0.8851 Acc: 0.5000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 6/20 Train Loss: 0.5294 Acc: 0.8182 Prec: 1.0000 Recall: 0.6923 F1: 0.8182\n",
            "Epoch 6/20 Val Loss: 0.9838 Acc: 0.0000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 7/20 Train Loss: 0.4912 Acc: 0.9091 Prec: 1.0000 Recall: 0.8462 F1: 0.9167\n",
            "Epoch 7/20 Val Loss: 1.0776 Acc: 0.0000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 8/20 Train Loss: 0.4648 Acc: 0.9545 Prec: 0.9286 Recall: 1.0000 F1: 0.9630\n",
            "Epoch 8/20 Val Loss: 1.0943 Acc: 0.0000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 9/20 Train Loss: 0.4592 Acc: 0.8636 Prec: 0.8571 Recall: 0.9231 F1: 0.8889\n",
            "Epoch 9/20 Val Loss: 1.1564 Acc: 0.0000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 10/20 Train Loss: 0.4095 Acc: 0.9091 Prec: 0.9231 Recall: 0.9231 F1: 0.9231\n",
            "Epoch 10/20 Val Loss: 1.4801 Acc: 0.0000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 11/20 Train Loss: 0.3159 Acc: 0.9545 Prec: 1.0000 Recall: 0.9231 F1: 0.9600\n",
            "Epoch 11/20 Val Loss: 1.6482 Acc: 0.0000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 12/20 Train Loss: 0.3115 Acc: 0.9545 Prec: 0.9286 Recall: 1.0000 F1: 0.9630\n",
            "Epoch 12/20 Val Loss: 1.7950 Acc: 0.0000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 13/20 Train Loss: 0.2720 Acc: 0.9091 Prec: 0.8667 Recall: 1.0000 F1: 0.9286\n",
            "Epoch 13/20 Val Loss: 1.9603 Acc: 0.0000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 14/20 Train Loss: 0.2842 Acc: 0.9545 Prec: 0.9286 Recall: 1.0000 F1: 0.9630\n",
            "Epoch 14/20 Val Loss: 2.0265 Acc: 0.0000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 15/20 Train Loss: 0.1629 Acc: 0.9545 Prec: 1.0000 Recall: 0.9231 F1: 0.9600\n",
            "Epoch 15/20 Val Loss: 2.0512 Acc: 0.0000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 16/20 Train Loss: 0.1312 Acc: 1.0000 Prec: 1.0000 Recall: 1.0000 F1: 1.0000\n",
            "Epoch 16/20 Val Loss: 2.1599 Acc: 0.0000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 17/20 Train Loss: 0.1260 Acc: 1.0000 Prec: 1.0000 Recall: 1.0000 F1: 1.0000\n",
            "Epoch 17/20 Val Loss: 2.5925 Acc: 0.0000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 18/20 Train Loss: 0.1665 Acc: 0.9545 Prec: 0.9286 Recall: 1.0000 F1: 0.9630\n",
            "Epoch 18/20 Val Loss: 2.6424 Acc: 0.0000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 19/20 Train Loss: 0.0672 Acc: 1.0000 Prec: 1.0000 Recall: 1.0000 F1: 1.0000\n",
            "Epoch 19/20 Val Loss: 2.8256 Acc: 0.0000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n",
            "Epoch 20/20 Train Loss: 0.0588 Acc: 1.0000 Prec: 1.0000 Recall: 1.0000 F1: 1.0000\n",
            "Epoch 20/20 Val Loss: 3.0188 Acc: 0.0000 Prec: 0.0000 Recall: 0.0000 F1: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model after training is done\n",
        "def save_model(model, path='model.pth'):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "# Call this after the last epoch or training completion\n",
        "save_model(model, 'deepfake_detector.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvIsE8oN1Dvy",
        "outputId": "daf56b23-4134-45ca-894c-ccdcd5900d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to deepfake_detector.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HoTnjGVE1FZp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}